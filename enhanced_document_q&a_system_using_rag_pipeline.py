# -*- coding: utf-8 -*-
"""Enhanced Document Q&A System using RAG Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNPMjPdFe3cyFsxJjrXXlu3VEfJYD0jh

Setup and Installation
"""

!pip install -q gradio gradio_pdf pypdf PyPDF2 pymupdf sentence-transformers transformers faiss-cpu torch numpy pandas
!pip install -q llama-index llama-index-readers-file llama-index-embeddings-huggingface llama-index-vector-stores-faiss
!pip install -q jedi>=0.16
!pip install --no-cache-dir llama-cpp-python==0.3.0 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123
!pip install -q pillow pytesseract

"""Core Imports and Configuration"""

import os
import torch
import gradio as gr
from gradio_pdf import PDF
from PyPDF2 import PdfReader
import fitz  # PyMuPDF
import numpy as np
from sentence_transformers import SentenceTransformer
import pytesseract
import faiss
from dataclasses import dataclass
from typing import List, Optional, Dict, Tuple

# LlamaIndex imports
from llama_index.core import Document, VectorStoreIndex, StorageContext
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter

from llama_cpp import Llama

use_gpu = torch.cuda.is_available()
n_gpu_layers = 32 if use_gpu else 0

model_path = "/content/mistral.gguf"
if not os.path.exists(model_path):
    !wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O {model_path}

llm = Llama(
    model_path=model_path,
    n_ctx=4096,
    n_gpu_layers=n_gpu_layers

)

"""Data Structures for Enhanced Document Management"""

def llm_generate(prompt: str, llm_instance: Llama,
                 max_tokens: int = 256,
                 temperature: float = 0.2,
                 stop=None) -> str:
    """
    Safe wrapper for llama-cpp >=0.3.0.
    Ensures prompt is passed correctly.
    """
    if not isinstance(llm_instance, Llama):
        raise TypeError(f"llm_instance is not a Llama model. Got {type(llm_instance)}")

    try:
        response = llm_instance(
            prompt,           # positional argument
            max_tokens=max_tokens,
            temperature=temperature,
            stop=stop
        )
        return response["choices"][0]["text"].strip()
    except Exception as e:
        print(f"LLM generation error: {e}")
        return ""

from google.colab import userdata
userdata.get('Huggingface')

embed_model = SentenceTransformer("all-MiniLM-L6-v2")
llama_embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

@dataclass
class PageInfo:
    page_num: int
    text: str
    doc_type: Optional[str] = None
    page_in_doc: int = 0

@dataclass
class LogicalDocument:
    doc_id: str
    doc_type: str
    page_start: int
    page_end: int
    text: str
    chunks: List[Dict] = None

@dataclass
class ChunkMetadata:
    chunk_id: str
    doc_id: str
    doc_type: str
    chunk_index: int
    page_start: int
    page_end: int
    text: str
    embedding: Optional[np.ndarray] = None

"""Document Intelligence Functions"""

def classify_document_type(text: str, llm_instance: Llama, max_length: int = 1500) -> str:
    sample = text[:max_length] if len(text) > max_length else text
    prompt = f"""
    Analyze this document and classify it into ONE of these categories:
    Resume, Contract, Mortgage Contract, Invoice, Pay Slip,
    Lender Fee Sheet, Land Deed, Bank Statement, Tax Document,
    Insurance, Report, Letter, Form, ID Document, Medical, Other

    Document sample:
    {sample}

    Respond with ONLY the category name.
    """
    try:
        doc_type = llm_generate(prompt, llm_instance)
        valid_types = [
            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',
            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',
            'Insurance', 'Report', 'Letter', 'Form', 'ID Document',
            'Medical', 'Other'
        ]
        for vt in valid_types:
            if doc_type.lower() == vt.lower():
                return vt
        return 'Other'
    except Exception as e:
        print(f"Classification error: {e}")
        return 'Other'

def detect_document_boundary(prev_text: str, curr_text: str,
                             llm_instance: Llama, current_doc_type: str = None) -> bool:
    if not prev_text or not curr_text:
        return False

    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text
    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text

    prompt = f"""
    Determine if these two pages are from the SAME document.
    Current document type: {current_doc_type or 'Unknown'}

    End of Previous Page:
    ...{prev_sample}

    Start of Current Page:
    {curr_sample}...

    Answer ONLY 'Yes' or 'No'.
    """
    try:
        result = llm_generate(prompt, llm_instance)
        return result.strip().lower().startswith("yes")
    except Exception as e:
        print(f"Boundary detection error: {e}")
        return True  # assume same if error

from PIL import Image
import io
import pytesseract

"""Advanced PDF Processing Pipeline"""

def extract_and_analyze_pdf(pdf_file, llm_instance=None) -> Tuple[List[PageInfo], List[LogicalDocument]]:
    print("üìñ Starting PDF extraction and analysis...")

    # Open PDF
    if isinstance(pdf_file, dict) and "content" in pdf_file:
        doc = fitz.open(stream=pdf_file["content"], filetype="pdf")
    elif hasattr(pdf_file, "read"):
        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    else:
        doc = fitz.open(pdf_file)

    pages_info = []
    for i, page in enumerate(doc):
        text = page.get_text()
        if not text.strip():
            # OCR fallback
            pix = page.get_pixmap()
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            text = pytesseract.image_to_string(img)
        pages_info.append(PageInfo(page_num=i, text=text))
    doc.close()

    print(f"‚úÖ Extracted {len(pages_info)} pages")

    # Document classification & boundary detection
    logical_docs = []
    current_doc_type = None
    current_doc_pages = []
    doc_counter = 0

    for i, page_info in enumerate(pages_info):
        if i == 0:
            current_doc_type = classify_document_type(page_info.text, llm_instance=llm_instance)
            page_info.doc_type = current_doc_type
            page_info.page_in_doc = 0
            current_doc_pages = [page_info]
        else:
            is_same = detect_document_boundary(pages_info[i-1].text, page_info.text, llm_instance = llm_instance,current_doc_type=current_doc_type)
            if is_same:
                page_info.doc_type = current_doc_type
                page_info.page_in_doc = len(current_doc_pages)
                current_doc_pages.append(page_info)
            else:
                logical_docs.append(LogicalDocument(
                    doc_id=f"doc_{doc_counter}",
                    doc_type=current_doc_type,
                    page_start=current_doc_pages[0].page_num,
                    page_end=current_doc_pages[-1].page_num,
                    text="\n\n".join([p.text for p in current_doc_pages])
                ))
                doc_counter += 1
                current_doc_type = classify_document_type(page_info.text, llm_instance=llm_instance)
                page_info.doc_type = current_doc_type
                page_info.page_in_doc = 0
                current_doc_pages = [page_info]

    # Last document
    if current_doc_pages:
        logical_docs.append(LogicalDocument(
            doc_id=f"doc_{doc_counter}",
            doc_type=current_doc_type,
            page_start=current_doc_pages[0].page_num,
            page_end=current_doc_pages[-1].page_num,
            text="\n\n".join([p.text for p in current_doc_pages])
        ))

    print(f"‚úÖ Identified {len(logical_docs)} logical documents")
    for ld in logical_docs:
        print(f"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end}")

    return pages_info, logical_docs

"""Intelligent Chunking with Metadata Preservation"""

def chunk_document_with_metadata(logical_doc: LogicalDocument,
                                chunk_size: int = 500,
                                overlap: int = 100) -> List[ChunkMetadata]:
    chunks_metadata = []
    words = logical_doc.text.split()
    if len(words) <= chunk_size:
        chunks_metadata.append(ChunkMetadata(
            chunk_id=f"{logical_doc.doc_id}_chunk_0",
            doc_id=logical_doc.doc_id,
            doc_type=logical_doc.doc_type,
            chunk_index=0,
            page_start=logical_doc.page_start,
            page_end=logical_doc.page_end,
            text=logical_doc.text,
            embedding=embed_model.encode(logical_doc.text)
        ))
    else:
        stride = chunk_size - overlap
        for i, start_idx in enumerate(range(0, len(words), stride)):
            end_idx = min(start_idx + chunk_size, len(words))
            chunk_text = ' '.join(words[start_idx:end_idx])
            chunk_page_start = logical_doc.page_start + int((start_idx / len(words)) * (logical_doc.page_end - logical_doc.page_start + 1))
            chunk_page_end = logical_doc.page_start + int((end_idx / len(words)) * (logical_doc.page_end - logical_doc.page_start + 1)) - 1
            chunks_metadata.append(ChunkMetadata(
                chunk_id=f"{logical_doc.doc_id}_chunk_{i}",
                doc_id=logical_doc.doc_id,
                doc_type=logical_doc.doc_type,
                chunk_index=i,
                page_start=chunk_page_start,
                page_end=chunk_page_end,
                text=chunk_text,
                embedding=embed_model.encode(chunk_text)
            ))
            if end_idx >= len(words):
                break
    return chunks_metadata

def chunk_with_llama_index(logical_doc: LogicalDocument,
                           chunk_size: int = 500,
                           chunk_overlap: int = 100) -> List[ChunkMetadata]:
    doc = Document(
        text=logical_doc.text,
        metadata={
            "doc_id": logical_doc.doc_id,
            "doc_type": logical_doc.doc_type,
            "page_start": logical_doc.page_start,
            "page_end": logical_doc.page_end,
            "source": f"{logical_doc.doc_type}_document"
        }
    )
    splitter = SentenceSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        paragraph_separator="\n\n",
        separator=" ",
    )
    nodes = splitter.get_nodes_from_documents([doc])
    chunks_metadata = []
    for i, node in enumerate(nodes):
        chunks_metadata.append(ChunkMetadata(
            chunk_id=f"{logical_doc.doc_id}_chunk_{i}",
            doc_id=logical_doc.doc_id,
            doc_type=logical_doc.doc_type,
            chunk_index=i,
            page_start=node.metadata.get("page_start", logical_doc.page_start),
            page_end=node.metadata.get("page_end", logical_doc.page_end),
            text=node.text,
            embedding=embed_model.encode(node.text)
        ))
    return chunks_metadata

def process_all_documents(logical_docs: List[LogicalDocument],
                         use_llama_index: bool = False) -> List[ChunkMetadata]:
    all_chunks = []
    for logical_doc in logical_docs:
        chunks = chunk_with_llama_index(logical_doc) if use_llama_index else chunk_document_with_metadata(logical_doc)
        logical_doc.chunks = chunks
        all_chunks.extend(chunks)
        print(f"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks")
    return all_chunks

"""Query Routing and Intelligent Retrieval"""

import json

def predict_query_document_type(query: str, llm_instance: Llama) -> Tuple[str, float]:
    prompt = f"""
    Analyze this query and predict which document type would most likely contain the answer.
    Query: "{query}"
    Choose the MOST LIKELY type from:
    Resume, Contract, Mortgage Contract, Invoice, Pay Slip,
    Lender Fee Sheet, Land Deed, Bank Statement, Tax Document,
    Insurance, Report, Letter, Form, ID Document, Medical, Other
    Respond in JSON format:
    {{ "type" : "DocumentType", "confidence" : 0.85}}
    Confidence should be between 0.0 and 1.0
    """
    try:
        response_text = llm_generate(prompt, llm_instance)


        if not response_text.strip():
            print("Query routing error: LLM returned an empty string.")
            return "Other", 0.0


        cleaned_response = response_text.strip().strip('`')
        if cleaned_response.startswith('json\n'):
            cleaned_response = cleaned_response[5:]


        result = json.loads(cleaned_response)

        doc_type = result.get("type", "Other")
        confidence = float(result.get("confidence", 0.5))
        return doc_type, confidence

    except json.JSONDecodeError as e:

        print(f"Query routing error (JSONDecodeError): {e}")
        print(f"  Raw LLM Output was: >>>{response_text}<<<")
        return "Other", 0.0

    except Exception as e:
        print(f"Query routing error (General Exception): {e}")
        return "Other", 0.0

class IntelligentRetriever:
    """
    Advanced retrieval system with metadata filtering and query routing.
    """

    def __init__(self):
        self.index = None
        self.chunks_metadata = []
        self.doc_type_indices = {}

    def build_indices(self, chunks_metadata: List[ChunkMetadata]):
        """
        Build FAISS indices with document type segregation.
        """
        print("üî® Building vector indices...")
        self.chunks_metadata = chunks_metadata


        texts = [chunk.text for chunk in chunks_metadata]
        embeddings = embed_model.encode(texts, show_progress_bar=True)


        for i, chunk in enumerate(chunks_metadata):
            chunk.embedding = embeddings[i]


        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)


        doc_types = set(chunk.doc_type for chunk in chunks_metadata)
        for doc_type in doc_types:
            type_indices = [i for i, chunk in enumerate(chunks_metadata)
                          if chunk.doc_type == doc_type]
            if type_indices:
                type_embeddings = embeddings[type_indices]
                type_index = faiss.IndexFlatL2(dim)
                type_index.add(type_embeddings)
                self.doc_type_indices[doc_type] = {
                    'index': type_index,
                    'mapping': type_indices
                }

        print(f"‚úÖ Indexed {len(chunks_metadata)} chunks across {len(doc_types)} document types")

    def retrieve(self, query: str, k: int = 4,
                filter_doc_type: Optional[str] = None,
                auto_route: bool = True,
                llm_instance_for_routing=None) -> List[Tuple[ChunkMetadata, float]]:
        """
        Retrieve relevant chunks with optional filtering and routing.
        Returns chunks with relevance scores.
        """
        query_embedding = embed_model.encode([query])


        if filter_doc_type and filter_doc_type in self.doc_type_indices:

            type_data = self.doc_type_indices[filter_doc_type]
            D, I = type_data['index'].search(query_embedding, k)

            chunk_indices = [type_data['mapping'][i] for i in I[0]]
            distances = D[0]
        elif auto_route:

            predicted_type, confidence = predict_query_document_type(query, llm_instance=llm_instance_for_routing)
            print(f"üéØ Query routed to: {predicted_type} (confidence: {confidence:.2f})")

            if confidence > 0.7 and predicted_type in self.doc_type_indices:

                type_data = self.doc_type_indices[predicted_type]
                D, I = type_data['index'].search(query_embedding, k)
                chunk_indices = [type_data['mapping'][i] for i in I[0]]
                distances = D[0]
            else:

                D, I = self.index.search(query_embedding, k)
                chunk_indices = I[0]
                distances = D[0]
        else:

            D, I = self.index.search(query_embedding, k)
            chunk_indices = I[0]
            distances = D[0]


        max_dist = max(distances) if len(distances) > 0 else 1.0
        scores = [(max_dist - d) / max_dist for d in distances]

        results = [(self.chunks_metadata[i], scores[idx])
                  for idx, i in enumerate(chunk_indices)]

        return results

"""#Enhanced Answer Generation with Source Attribution"""

def generate_answer_with_sources(query: str,
                                 retrieved_chunks: List[Tuple['ChunkMetadata', float]],
                                 llm_instance: Llama) -> Dict:
    if not retrieved_chunks:
        return {
            'answer': "I couldn't find relevant information to answer your question.",
            'sources': [],
            'confidence': 0.0,
            'chunks_used': 0
        }

    context_parts = []
    sources = []

    for chunk_meta, score in retrieved_chunks:
        context_parts.append(f"[From {chunk_meta.doc_type}, Pages {chunk_meta.page_start}-{chunk_meta.page_end}]")
        context_parts.append(chunk_meta.text)
        context_parts.append("")
        sources.append({
            'doc_type': chunk_meta.doc_type,
            'pages': f"{chunk_meta.page_start}-{chunk_meta.page_end}",
            'relevance': f"{score:.2%}",
            'preview': (chunk_meta.text[:100] + "...") if len(chunk_meta.text) > 100 else chunk_meta.text
        })

    context = "\n".join(context_parts)

    prompt = f"""
You are a helpful AI assistant. Use the provided context to answer the question.
Be specific and cite which document type and pages support your answer.

Context:
{context}

Question: {query}

Instructions:
1. Answer based ONLY on the provided context
2. Mention which document type(s) contain the information
3. Be concise but complete
4. If the context doesn't contain enough information, say so

Answer:
"""
    try:
        answer = llm_generate(prompt, llm_instance)
        avg_score = sum(score for _, score in retrieved_chunks) / len(retrieved_chunks)
        return {
            'answer': answer,
            'sources': sources,
            'confidence': avg_score,
            'chunks_used': len(retrieved_chunks)
        }
    except Exception as e:
        print(f"Answer generation error: {e}")
        return {
            'answer': f"Error generating answer: {e}",
            'sources': sources,
            'confidence': 0.0,
            'chunks_used': len(retrieved_chunks)
        }

"""Enhanced Answer Generation with Source Attribution"""

from typing import Optional, List, Dict, Tuple
from datetime import datetime

class EnhancedDocumentStore:
    """
    Manages the complete document processing and retrieval pipeline.
    """

    def __init__(self, llm_model_instance=None, model_path: str = None, n_ctx: int = 1024, n_gpu_layers: int = 0):
        self.pages_info: List[PageInfo] = []
        self.logical_docs: List[LogicalDocument] = []
        self.chunks_metadata: List[ChunkMetadata] = []
        self.retriever: IntelligentRetriever = IntelligentRetriever()
        self.is_ready: bool = False
        self.processing_stats: Dict = {}
        self.filename: Optional[str] = None


        if isinstance(llm_model_instance, Llama):
            self.llm_model = llm_model_instance
        else:
            print(f"WARNING: Provided llm_model_instance is not a Llama object (type: {type(llm_model_instance)}). Attempting to re-initialize Llama.")
            if model_path and os.path.exists(model_path):
                try:
                    self.llm_model = Llama(
                        model_path=model_path,
                        n_ctx=n_ctx,
                        n_gpu_layers=n_gpu_layers
                    )
                    print("Llama model successfully re-initialized within EnhancedDocumentStore.")
                except Exception as e:
                    print(f"ERROR: Failed to re-initialize Llama model: {e}")
                    self.llm_model = None
            else:
                print("ERROR: model_path not provided or model file not found for re-initialization. LLM will be None.")
                self.llm_model = None

    def process_pdf(self, pdf_file, filename: str = "document.pdf") -> Tuple[bool, Dict]:
        """
        Complete PDF processing pipeline: extract, chunk, and index.
        """
        self.filename = filename
        self.is_ready = False
        start_time = datetime.now()

        if self.llm_model is None:
            return False, {'error': 'LLM model is not initialized or failed to load.'}

        try:
            # Extract and analyze PDF
            self.pages_info, self.logical_docs = extract_and_analyze_pdf(pdf_file, llm_instance=self.llm_model)

            # Chunk documents with metadata
            self.chunks_metadata = process_all_documents(self.logical_docs, use_llama_index=True)


            self.retriever.build_indices(self.chunks_metadata)


            process_time = (datetime.now() - start_time).total_seconds()
            self.processing_stats = {
                'filename': filename,
                'total_pages': len(self.pages_info),
                'documents_found': len(self.logical_docs),
                'total_chunks': len(self.chunks_metadata),
                'document_types': list({doc.doc_type for doc in self.logical_docs}),
                'processing_time': f"{process_time:.1f}s"
            }

            self.is_ready = True
            return True, self.processing_stats

        except Exception as e:
            return False, {'error': str(e)}

    def query(self, question: str, filter_type: Optional[str] = None,
              auto_route: bool = True, k: int = 4) -> Dict:
        """
        Query the document store.
        Returns the answer, sources, confidence, and metadata about filter used.
        """
        if not self.is_ready or self.llm_model is None:
            return {
                'answer': "Please upload and process a PDF first, and ensure LLM is loaded.",
                'sources': [],
                'confidence': 0.0,
                'chunks_used': 0,
                'filter_used': None
            }


        retrieved = self.retriever.retrieve(
            query=question,
            filter_doc_type=filter_type,
            auto_route=auto_route,
            k=k,
            llm_instance_for_routing=self.llm_model
        )


        result = generate_answer_with_sources(question, retrieved, llm_instance=self.llm_model)
        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')

        return result

    def get_document_structure(self) -> List[Dict]:
        """
        Get the document structure for UI display.
        Returns a list of documents with basic metadata.
        """
        if not self.logical_docs:
            return []

        structure = []
        for doc in self.logical_docs:
            structure.append({
                'id': doc.doc_id,
                'type': doc.doc_type,
                'pages': f"{doc.page_start + 1}-{doc.page_end + 1}",  # 1-indexed for UI
                'chunks': len(doc.chunks) if doc.chunks else 0,
                'preview': (doc.text[:200] + "...") if len(doc.text) > 200 else doc.text
            })

        return structure

"""Gradio Interface with Enhanced Features"""

import gradio as gr



doc_store = EnhancedDocumentStore(
    llm_model_instance=llm,
    model_path=model_path,
    n_ctx=1024,
    n_gpu_layers=n_gpu_layers
)



def process_pdf_handler(pdf_file) -> Tuple[str, str, gr.update]:
    """Handle PDF upload and processing."""
    if pdf_file is None:
        return "‚ö†Ô∏è Please upload a PDF file.", "", gr.update(choices=["All"], value="All")

    success, stats = doc_store.process_pdf(pdf_file, filename=getattr(pdf_file, 'name', 'document.pdf'))

    if success:
        status_msg = f"""
‚úÖ **Successfully Processed:**
- üìÑ File: {stats['filename']}
- üìë Pages: {stats['total_pages']}
- üìö Documents Found: {stats['documents_found']}
- üß© Chunks Created: {stats['total_chunks']}
- üè∑Ô∏è Types: {', '.join(stats['document_types'])}
- ‚è±Ô∏è Time: {stats['processing_time']}
"""
        structure_list = doc_store.get_document_structure()
        structure_display = "\n".join([
            f"‚Ä¢ **{doc['type']}** (Pages {doc['pages']}): {doc['chunks']} chunks"
            for doc in structure_list
        ])
        doc_types = ["All"] + stats['document_types']
        return status_msg, structure_display, gr.update(choices=doc_types, value="All")
    else:
        return f"‚ùå Error: {stats.get('error', 'Unknown error')}", "", gr.update(choices=["All"], value="All")

def chat_handler(message, history, doc_filter, auto_route, num_chunks):
    if not doc_store.is_ready:
        response = "üìö Please upload and process a PDF document first." # Added check for LLM load status implicitly
        return  history + [
        {'role': 'user', 'content': message},
        {'role': 'assistant', 'content': response}
    ]




    result = doc_store.query(
        question=message,
        filter_type=None if doc_filter == "All" else doc_filter,
        auto_route=auto_route,
        k=num_chunks
    )

    response = result[ 'answer' ]
    return history + [{'role': 'user', 'content': message}, {'role': 'assistant', 'content': response}]

def create_interface() -> gr.Blocks:
    with gr.Blocks(title="Enhanced Document Q&A") as demo:
        gr.Markdown("""
# üöÄ Enhanced Document Q&A System
### Intelligent Multi-Document Analysis with Advanced RAG Pipeline
""")

        with gr.Row():

            with gr.Column(scale=2):
                pdf_input = gr.File(
                    label="üìÑ Upload PDF Document",
                    file_types=[".pdf"],
                    type="filepath"
                )
                with gr.Row():
                    process_btn = gr.Button("üîÑ Process Document", variant="primary")
                    clear_all_btn = gr.Button("üóëÔ∏è Clear All", variant="secondary")


            with gr.Column(scale=1):
                gr.Markdown("### üìä Document Info")
                status_output = gr.Markdown("‚è≥ Waiting for PDF upload...")
                structure_output = gr.Markdown("", label="Document Structure")

                gr.Markdown("### ‚öôÔ∏è Settings")
                doc_filter = gr.Dropdown(
                    choices=["All"],
                    value="All",
                    label="üè∑Ô∏è Document Type Filter"
                )
                auto_route = gr.Checkbox(value=True, label="üéØ Auto-Route Queries")
                num_chunks = gr.Slider(minimum=1, maximum=10, value=4, step=1, label="üìä Chunks to Retrieve")


            with gr.Column(scale=2):
                gr.Markdown("### üí¨ Ask Questions")
                chatbot = gr.Chatbot(height=500)

                with gr.Row():
                    msg_input = gr.Textbox(placeholder="e.g., What are the payment terms?", show_label=False)
                    send_btn = gr.Button("üì§ Send", variant="primary")

                with gr.Row():
                    clear_chat_btn = gr.Button("üóëÔ∏è Clear Chat")
                    example_btn1 = gr.Button("üìù What's the summary?")
                    example_btn2 = gr.Button("üí∞ Find amounts")


        status_bar = gr.Markdown("**Status:** Ready | **Documents:** 0 | **Chunks:** 0")


        def update_status_bar() -> str:
            if doc_store.is_ready:
                stats = doc_store.processing_stats
                return f"**Status:** ‚úÖ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)}"
            return "**Status:** Ready | **Documents:** 0 | **Chunks:** 0"

        def clear_all():
            global doc_store
            doc_store = EnhancedDocumentStore(
                llm_model_instance=llm,
                model_path=model_path,
                n_ctx=1024,
                n_gpu_layers=n_gpu_layers
            )
            return (
                None,
                "‚è≥ Waiting for PDF upload...",
                "",
                gr.update(choices=["All"], value="All"),
                [],
                "",
                update_status_bar()
            )

        def process_pdf_with_status(pdf_file):
            status, structure, filter_update = process_pdf_handler(pdf_file)
            return status, structure, filter_update, update_status_bar()

        def chat_with_status(message, history, doc_filter_val, auto_route_val, num_chunks_val):
            new_history = chat_handler(message, history, doc_filter_val, auto_route_val, num_chunks_val)
            return new_history, update_status_bar()

        def ask_summary(history):

            return chat_handler(
                "Can you provide a summary of the main points in this document?",
                history, doc_filter.value, auto_route.value, num_chunks.value
            )

        def ask_amounts(history):

            return chat_handler(
                "What are all the monetary amounts or financial figures mentioned?",
                history, doc_filter.value, auto_route.value, num_chunks.value
            )


        process_btn.click(
            fn=process_pdf_with_status,
            inputs=[pdf_input],
            outputs=[status_output, structure_output, doc_filter, status_bar]
        )

        clear_all_btn.click(
            fn=clear_all,
            outputs=[pdf_input, status_output, structure_output, doc_filter, chatbot, msg_input, status_bar]
        )

        msg_input.submit(
            fn=chat_with_status,
            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],
            outputs=[chatbot, status_bar]
        ).then(lambda: "", outputs=[msg_input])

        send_btn.click(
            fn=chat_with_status,
            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],
            outputs=[chatbot, status_bar]
        ).then(lambda: "", outputs=[msg_input])

        clear_chat_btn.click(lambda: [], outputs=[chatbot])
        example_btn1.click(fn=ask_summary, inputs=[chatbot], outputs=[chatbot])
        example_btn2.click(fn=ask_amounts, inputs=[chatbot], outputs=[chatbot])
        pdf_input.change(fn=process_pdf_with_status, inputs=[pdf_input], outputs=[status_output, structure_output, doc_filter, status_bar])

    return demo

demo = create_interface()
demo.launch(theme=gr.themes.Soft(), share=True,debug=True)